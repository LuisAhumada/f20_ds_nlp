{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework #1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will be analyzing job descriptions from a number of different fields. The thought is that these job descriptions might contain both jargon word ands phrases.\n",
    "\n",
    "The challenge here will be to analyze the text of the included job descriptions, but to also compare the words and phrases there with a reference set. In this case, we will use Reuters news articles as a background corpus to compare our possible jargon text with.\n",
    "\n",
    "This homework will require that you read in the text of the job descriptions and then tokenize them. You will then need to take the tokens and compare them to the Reuters as both individual tokens and also as bigrams.\n",
    "\n",
    "You need not look at the frequency of the terms. We are aiming for just term differences, so simply reporting back the tokens that are only in the job descriptions will be sufficient. One key thing to consider here is what kind of tokens will you want to report on. For example, the job descriptions might contain numbers and other things. Generally, you'd not want to report back numbers. Also, you might want to consider lowercasing things. \n",
    "\n",
    "If you'd like you can also try to stem or lemmatize the text.\n",
    "\n",
    "The code has been built around using NLTK, but you could just as easily do this with Spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will import necessary libraries for using NLTK\n",
    "\n",
    "import nltk.data\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from nltk.util import bigrams \n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence_tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "treebank_tokenizer = TreebankWordTokenizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_base = \"/Users/teacher/repos/s20_ds_nlp/homeworks/homework_1/data/\"\n",
    "\n",
    "\n",
    "####\n",
    "# Notice: We are reusing code from class notes... remember these kind of building blocks\n",
    "####\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = open(filename , encoding='utf-8').read()\n",
    "    return input_file_text\n",
    "\n",
    "    \n",
    "def read_directory_files(directory):\n",
    "    file_texts = []\n",
    "    files = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    for f in files:\n",
    "        file_text = read_file(join(directory, f))\n",
    "        print(file_text)\n",
    "        file_texts.append({\"file\":f, \"content\": file_text })\n",
    "    return file_texts\n",
    "    \n",
    "# here we will generate the list that contains all the files and their contents\n",
    "text_corpus = read_directory_files(dir_base)\n",
    "print(text_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# You will need to work on filling out the content of this method. \n",
    "###\n",
    "\n",
    "def process_description(job_description_object):\n",
    "    job_description = job_description_object[\"content\"]\n",
    "    print(job_description)\n",
    "    \n",
    "    # take the job description text, and tokenize it\n",
    "    # you could also remove numbers and other noise tokens here too\n",
    "    # also, you might generate bigrams here as well\n",
    "    \n",
    "    return # the later function assumes you are returning a list of terms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This loop will simply apply your method to all the job descriptions\n",
    "all_job_description_words = []\n",
    "for job_description in text_corpus:\n",
    "    all_job_description_words.extend(process_description(job_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the corpus we work from\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = len(reuters.fileids())\n",
    "#  this has a large number of files... \n",
    "# you might wish to limit the number of documents you use while developing your technique \n",
    "# ex. reuters.fileids()[0:25]\n",
    "\n",
    "all_reuters_words = []\n",
    "\n",
    "# this will only iterate over the first 25 documents, \n",
    "# for the real submission you will need to run across more documents\n",
    "# perhaps 250 documents, or all of them\n",
    "for doc_id in reuters.fileids()[0:25]: \n",
    "    # this doc_text variable will give you a text version of the news article. This could be tokenized.\n",
    "    reuters_text = reuters.open(doc_id).read()\n",
    "    print(reuters_text)\n",
    "    # here you could perhaps run the same job description processing method\n",
    "    # then you could simply add the output to the all_reuters_words list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here you will want to find ways to compare the words in the job \n",
    "# descriptions and the reuters text\n",
    "# you might consider using Python's set capabilities to intersect things\n",
    "# also, you might just iterate over the job description words to see if \n",
    "# they are in the reuters word list\n",
    "\n",
    "# all_job_description_words\n",
    "# all_reuters_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this cell, please put a short writeup of your approach and comments on your results. The goal here is to explain how well you think your method worked based on looking at some of your output data. Additionally, please describe things you might do fifferently or ways in which you might improve the process if you were given more time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
